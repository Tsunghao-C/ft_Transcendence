networks:
  frontend_net:
    driver: bridge
  backend_net:
    driver: bridge
    # internal: true
    internal: false # in order to send 2FA
  monitoring_net:
    driver: bridge

volumes:
  vault_debian_shared_data:
    name: vault_debian_data
    driver: local
    driver_opts:
      device: ./security/vault_debian/volumes/shared_data
      o: bind
      type: none
  vault_debian_file:
    name: vault_file
    driver: local
    driver_opts:
      device: ./security/vault_debian/volumes/file
      o: bind
      type: none
  nginx_temp: {}
  auth_data: {}
  user_data: {}
  static_volume: {} # Added by Ben
  redis_data: {}
  game_data: {}
  match_data: {}
  prometheus_data: {}
  grafana_data: {}
  filebeat_registry: {}
  es_data:
    driver: local

services:
  # Security Layer
  vault_debian:
    container_name: vault_debian
    build:
      context: ./security/vault_debian
      dockerfile: Dockerfile
    cap_add:
      - IPC_LOCK
    ports:
      - "8200:8200"
    environment:
      VAULT_ADDR: "https://127.0.0.1:8200"
      VAULT_API_ADDR: "https://127.0.0.1:8200"
      VAULT_SKIP_VERIFY: "true"
    volumes:
      - vault_debian_shared_data:/vault/shared_data
      - vault_debian_file:/vault/file
    stop_signal: SIGTERM # DTIW

  ## [Alex] to finish WAF later after major modules are tested working

  # waf:
  #   container_name: waf
  #   build:
  #     context: ./security/waf
  #   ports:
  #     - "8443:443"
  #     - "8080:80"
  #   volumes:
  #     - ./security/waf/rules:/etc/modsecurity.d/rules
  #     - ./security/waf/nginx:/etc/nginx/conf.d
  #   depends_on:
  #     nginx:
  #       condition: service_healthy
  #   networks:
  #     - frontend_net
  #   healthcheck:
  #     test: ["CMD", "nginx", "-t"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #   restart: unless-stopped

  # two_fa:
  #   container_name: 2fa
  #   build: ./security/2fa
  #   ports:
  #     - "8082:8082"
  #   networks:
  #     - backend_net
  #   restart: unless-stopped

  # Frontend Layer
  ## [Theo] & [Alex]
  nginx:
    container_name: nginx
    build:
      context: ./frontend/nginx
      dockerfile: Dockerfile
    volumes:
      - ./frontend/nginx/spa_files:/usr/share/nginx/html
      - static_volume:/app/static:ro
      - ./frontend/nginx/logs:/var/log/nginx:rw
      - ./frontend/nginx/modsec:/var/log/modsec:rw
    ports:
      - "8443:443"
      - "8080:80"
    networks:
      - frontend_net
      - backend_net
      - monitoring_net
    environment:
      DEBUG: ${DEBUG}
      SECRET_KEY: ${SECRET_KEY}
      DJANGO_ALLOWED_HOSTS: ${DJANGO_ALLOWED_HOSTS}
      SQL_ENGINE: ${SQL_ENGINE}
      SQL_USER: ${SQL_USER}
      SQL_PASSWORD: ${SQL_PASSWORD}
      SQL_PORT: ${SQL_PORT}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      - kibana
    healthcheck:
      test: ["CMD", "curl", "-k", "https://localhost:443/health" ]
      interval: 2s
      timeout: 2s
      retries: 1
    restart: unless-stopped

  # spa:
  #   container_name: spa
  #   build:
  #     context: ./frontend/spa
  #   volumes:
  #     - ./frontend:/app
  #     - /app/node_modules
  #   environment:
  #     - REACT_APP_API_URL=http://localhost/api
  #     - REACT_APP_WS_URL=ws://localhost/ws
  #   networks:
  #     - frontend_net
  #   restart: unless-stopped

  # Auth Layer
  # auth_service:
  #   container_name: auth_service
  #   build: ./auth-service
  #   user: "1000:1000"
  #   ports:
  #     - "8001:8001"   # only for development stage, need to close after
  #   environment:
  #     - DATABASE_URL=postgresql://postgres:${AUTH_DB_PASSWORD}@auth_db:5432/authdb
  #     - VAULT_ADDR=http://vault:8200
  #   depends_on:
  #     - auth_db
  #     - vault
  #   networks:
  #     - backend_net
  #   restart: unless-stopped

  # auth_db:
  #   container_name: auth_db
  #   image: postgres:14-alpine
  #   environment:
  #     - POSTGRES_DB=authdb
  #     - POSTGRES_PASSWORD=${AUTH_DB_PASSWORD}
  #   volumes:
  #     - auth_data:/var/lib/postgresql/data
  #   user: "999:999"
  #   networks:
  #     - backend_net
  #   restart: unless-stopped

  # User Service
  ## [Ben]

  redis:
    container_name: redis
    image: redis:alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6380:6379"     # only for development stage, need to close after
    networks:
      - backend_net
    restart: unless-stopped

  game_service:
     container_name: game_service
     build:
       context: ./game-service
       dockerfile: Dockerfile
     image: game_service
     pull_policy: never
     volumes:
       - static_volume:/app/static # Match the path
     expose:
       - 8004
    #  ports:
    #    - "8004:8004"     # only for development stage, need to close after
     environment:
       # - DATABASE_URL=postgresql://postgres:${GAME_DB_PASSWORD}@game_db:5434/gamedb
       # - MATCH_DB_URL=postgresql://postgres:${MATCH_DB_PASSWORD}@match_db:5432/matchdb
       # - VAULT_ADDR=http://vault:8200
       # - VAULT_TOKEN=${VAULT_DEV_ROOT_TOKEN}
       DEBUG: ${DEBUG}
       SECRET_KEY: ${SECRET_KEY}
       DJANGO_ALLOWED_HOSTS: ${DJANGO_ALLOWED_HOSTS}
       SQL_ENGINE: ${SQL_ENGINE}
       GAME_DB_DATABASE: ${GAME_DB_DATABASE}
       USER_DB_DATABASE: ${USER_DB_DATABASE}
       SQL_USER: ${SQL_USER}
       SQL_PASSWORD: ${SQL_PASSWORD}
       SQL_PORT: ${SQL_PORT}
       SQL_HOST: game_db
       USER_DB_HOST: user_db
       LDB_UPDATE_TIMER: ${LDB_UPDATE_TIMER}
     depends_on:
       - game_db
       - user_db
     networks:
       - backend_net
     restart: unless-stopped

  game_db:
     container_name: game_db
     image: postgres:14-alpine
     environment:
       - POSTGRES_DB=game_db
       - POSTGRES_USER=${SQL_USER}
       - POSTGRES_PASSWORD=${SQL_PASSWORD}
     volumes:
       - game_data:/var/lib/postgresql/data
     networks:
       - backend_net
     restart: unless-stopped

  user_db:
     container_name: user_db
     image: postgres:14-alpine
     environment:
       - POSTGRES_DB=user_db
       - POSTGRES_USER=${SQL_USER}
       - POSTGRES_PASSWORD=${SQL_PASSWORD}
     volumes:
       - user_data:/var/lib/postgresql/data
       - static_volume:/app/static
     networks:
       - backend_net
     restart: unless-stopped

  # match_db:
  #   container_name: match_db
  #   image: postgres:14-alpine
  #   environment:
  #     - POSTGRES_DB=matchdb
  #     - POSTGRES_PASSWORD=${MATCH_DB_PASSWORD}
  #   volumes:
  #     - match_data:/var/lib/postgresql/data
  #   ports:
  #     - "5435:5432"     # only for development stage, need to close after
  #   user: "999:999"
  #   networks:
  #     - backend_net
  #   restart: unless-stopped

  # Monitoring Stack
  # prometheus:
  #   container_name: prometheus
  #   image: prom/prometheus
  #   user: "1000:1000"
  #   ports:
  #     - "9090:9090"   # only for development stage
  #   volumes:
  #     - ./monitoring/prometheus:/etc/prometheus
  #     - prometheus_data:/prometheus
  #   networks:
  #     - monitoring_net
  #   restart: unless-stopped

  # grafana:
  #   container_name: grafana
  #   image: grafana/grafana
  #   user: "1000:1000"
  #   ports:
  #     - "3001:3001"
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
  #   volumes:
  #     - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
  #     - grafana_data:/var/lib/grafana
  #   networks:
  #     - monitoring_net
  #   restart: unless-stopped

  # cadvisor:
  #   container_name: cadvisor
  #   image: gcr.io/cadvisor/cadvisor
  #   user: "1000:1000"
  #   volumes:
  #     - /:/rootfs:ro
  #     - /var/run:/var/run:rw
  #     - /sys:/sys:ro
  #     - /var/lib/docker/:/var/lib/docker:ro
  #   networks:
  #     - monitoring_net
  #   restart: unless-stopped

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    environment:
      # Cluster configuration
      - node.name=es01
      - cluster.name=pong-game-logs
      - discovery.type=single-node
      - network.host=0.0.0.0
      # Basic settings
      - bootstrap.memory_lock=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      # Disk allocation setting
      - "cluster.routing.allocation.disk.threshold_enabled=true"
      - "cluster.routing.allocation.disk.watermark.low=85%"
      - "cluster.routing.allocation.disk.watermark.high=90%"
      - "cluster.routing.allocation.disk.watermark.flood_stage=95%"
      # Security settings (for development)
      - xpack.security.enabled=false
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
    volumes:
      - es_data:/usr/share/elasticsearch/data
      - ./monitoring/elk/elasticsearch/setup:/setup
    entrypoint: >
      sh -c '
        /usr/local/bin/docker-entrypoint.sh &
        ELASTIC_PID=$!
        sleep 10 &&
        /setup/init.sh &
        wait $ELASTIC_PID'
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    networks:
      - monitoring_net
    # healthcheck:
    #   test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -vq '\"status\":\"red\"'"]
    #   interval: 5s
    #   timeout: 10s
    #   retries: 1
    restart: unless-stopped

  logstash:
    container_name: logstash
    image: docker.elastic.co/logstash/logstash:${STACK_VERSION}
    volumes:
      - ./monitoring/elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
      # ELASTIC_HOST: elasticsearch
    networks:
      - monitoring_net
    depends_on:
      - elasticsearch
    # healthcheck:
    #   test: ["CMD", "bin/logstash", "-t"]
    #   interval: 3s
    #   timeout: 3s
    #   retries: 1
    restart: unless-stopped

  kibana:
    container_name: kibana
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    volumes:
      - ./monitoring/elk/kibana/dashboards:/usr/share/kibana/dashboards
      - ./monitoring/elk/kibana/setup:/usr/share/kibana/setup
    environment:
      # Elasticsearch connection
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=${ELASTIC_USERNAME}
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      # Server settings
      - SERVER_HOST=0.0.0.0
      - SERVER_NAME=kibana
      - SERVER_BASEPATH=/kibana
      - SERVER_REWRITEBASEPATH=true
      # Security settings
      - XPACK_SECURITY_ENABLED=false
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      # Performance settings
      - ELASTICSEARCH_REQUESTTIMEOUT=90000  # Added timeout
      - OPTIMIZATION_PROFILING=false
      # Other settings
      - TELEMETRY_ENABLED=false
      - ELASTIC_APM_ACTIVE=false
    networks:
      - monitoring_net
    depends_on:
      - elasticsearch
    # healthcheck:
    #   test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q '200 OK'"]
    #   interval: 20s
    #   timeout: 10s
    #   retries: 3
    entrypoint: >
      sh -c '
        /usr/share/kibana/setup/import-dashboards.sh &
        /usr/local/bin/kibana-docker'
    restart: unless-stopped

  filebeat:
    container_name: filebeat
    image: docker.elastic.co/beats/filebeat:${STACK_VERSION}
    user: root  # Required to read logs
    volumes:
      - ./monitoring/elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
      # Mount all log directories
      - ./frontend/nginx/logs:/var/log/nginx:ro
      - ./frontend/nginx/modsec:/var/log/modsec:ro
      - filebeat_registry:/usr/share/filebeat/data
      # - ./auth-service/logs:/var/log/auth:ro
    entrypoint: >
      sh -c '
      chmod go-w /usr/share/filebeat/filebeat.yml &&
      filebeat -e --strict.perms=false'
    networks:
      - monitoring_net
    depends_on:
      - logstash
